name: Daily Spark Optimization

on:
  schedule:
    - cron: "30 11 * * *"   # daily 11:30 UTC ≈ 7:30 AM ET
  workflow_dispatch: {}

permissions:
  contents: write
  pull-requests: write

jobs:
  optimize:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Java & Spark (local mode tests)
        run: |
          sudo apt-get update
          sudo apt-get install -y openjdk-11-jdk wget
          
          # Try multiple Spark versions in case one is unavailable
          SPARK_VERSIONS=("3.5.6" "3.5.5" "3.5.4" "3.5.3")
          SPARK_VER=""
          
          for version in "${SPARK_VERSIONS[@]}"; do
            echo "Trying Spark version: $version"
            if wget --spider https://downloads.apache.org/spark/spark-${version}/spark-${version}-bin-hadoop3.tgz 2>/dev/null; then
              SPARK_VER=$version
              echo "Found available Spark version: $SPARK_VER"
              break
            fi
          done
          
          if [ -z "$SPARK_VER" ]; then
            echo "❌ No available Spark version found. Using PySpark from pip instead."
            echo "SPARK_HOME=" >> $GITHUB_ENV
            echo "JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> $GITHUB_ENV
          else
            echo "📦 Downloading Spark $SPARK_VER..."
            wget https://downloads.apache.org/spark/spark-${SPARK_VER}/spark-${SPARK_VER}-bin-hadoop3.tgz
            tar xf spark-${SPARK_VER}-bin-hadoop3.tgz
            sudo mv spark-${SPARK_VER}-bin-hadoop3 /opt/spark
            echo "SPARK_HOME=/opt/spark" >> $GITHUB_ENV
            echo "JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> $GITHUB_ENV
            echo "$SPARK_HOME/bin" >> $GITHUB_PATH
          fi

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install openai>=1.35.0 tiktoken>=0.7.0 pygithub>=2.3.0

      - name: Run LLM optimization
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: python scripts/optimize_with_llm.py

      - name: Run tests
        env:
          PYSPARK_PYTHON: python
        run: |
          if [ -d "tests" ]; then
            # Run all tests - PySpark tests will be skipped if not available
            python -m pytest tests/ -q --tb=short || echo "Some tests failed or were skipped"
            echo "Tests completed."
          else
            echo "No tests found; skipping."
          fi

      - name: Configure git
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

      - name: Commit changes (if any)
        id: commit
        run: |
          if [ -n "$(git status --porcelain)" ]; then
            BR="llm-optimize-$(date +%Y%m%d)"
            git checkout -b "$BR"
            git add -A
            git commit -m "LLM: daily Spark optimizations"
            git push --set-upstream origin "$BR"
            echo "branch=$BR" >> "$GITHUB_OUTPUT"
          else
            echo "branch=" >> "$GITHUB_OUTPUT"
            echo "No changes to commit."
          fi

      - name: Open PR (if branch exists)
        if: steps.commit.outputs.branch != ''
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          TITLE="LLM: Daily Spark optimizations ($(date +%Y-%m-%d))"
          BODY_FILE="llm_pr_body.md"
          if [ ! -f "$BODY_FILE" ]; then
            echo "Automated daily optimization suggestions." > "$BODY_FILE"
          fi
          gh pr create \
            --base main \
            --head "${{ steps.commit.outputs.branch }}" \
            --title "$TITLE" \
            --body-file "$BODY_FILE"
