name: Daily Spark Optimization

on:
  schedule:
    - cron: "30 11 * * *"   # daily 11:30 UTC â‰ˆ 7:30 AM ET
  workflow_dispatch: {}

permissions:
  contents: write
  pull-requests: write

jobs:
  optimize:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Java & Spark (local mode tests)
        run: |
          sudo apt-get update
          sudo apt-get install -y openjdk-11-jdk wget
          
          # Try multiple Spark versions in case one is unavailable
          SPARK_VERSIONS=("3.5.6" "3.5.5" "3.5.4" "3.5.3")
          SPARK_VER=""
          
          for version in "${SPARK_VERSIONS[@]}"; do
            echo "Trying Spark version: $version"
            if wget --spider https://downloads.apache.org/spark/spark-${version}/spark-${version}-bin-hadoop3.tgz 2>/dev/null; then
              SPARK_VER=$version
              echo "Found available Spark version: $SPARK_VER"
              break
            fi
          done
          
          if [ -z "$SPARK_VER" ]; then
            echo "âŒ No available Spark version found. Using PySpark from pip instead."
            echo "SPARK_HOME=" >> $GITHUB_ENV
            echo "JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> $GITHUB_ENV
          else
            echo "ðŸ“¦ Downloading Spark $SPARK_VER..."
            wget https://downloads.apache.org/spark/spark-${SPARK_VER}/spark-${SPARK_VER}-bin-hadoop3.tgz
            tar xf spark-${SPARK_VER}-bin-hadoop3.tgz
            sudo mv spark-${SPARK_VER}-bin-hadoop3 /opt/spark
            echo "SPARK_HOME=/opt/spark" >> $GITHUB_ENV
            echo "JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> $GITHUB_ENV
            echo "$SPARK_HOME/bin" >> $GITHUB_PATH
            
            # Clean up downloaded files
            rm -f spark-${SPARK_VER}-bin-hadoop3.tgz
          fi

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install openai>=1.35.0 tiktoken>=0.7.0 pygithub>=2.3.0

      - name: Run LLM optimization
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: python scripts/optimize_with_llm.py

      - name: Run tests
        env:
          PYSPARK_PYTHON: python
        run: |
          if [ -d "tests" ]; then
            # Run all tests - PySpark tests will be skipped if not available
            python -m pytest tests/ -q --tb=short || echo "Some tests failed or were skipped"
            echo "Tests completed."
          else
            echo "No tests found; skipping."
          fi

      - name: Configure git
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

      - name: Clean up large files
        run: |
          # Remove Spark binary files and other unwanted files before committing
          find . -name "spark-*.tgz" -type f -delete || true
          find . -name "spark-*-bin-*" -type d -exec rm -rf {} + || true
          find . -name "*.tgz" -type f -delete || true
          
          # Remove any files larger than 50MB (safety check)
          find . -type f -size +50M -delete || true
          
          # Clean up any temporary files
          find . -name "*.tmp" -type f -delete || true
          find . -name "*.temp" -type f -delete || true
          
          echo "Cleanup completed."

      - name: Commit changes (if any)
        id: commit
        run: |
          
          # Check if there are any meaningful changes to commit
          # Only commit Python files, markdown files, and configuration files
          if [ -n "$(git status --porcelain)" ]; then
            BR="llm-optimize-$(date +%Y%m%d)"
            git checkout -b "$BR"
            
            # Only add specific file types that should be committed
            git add "*.py" "*.md" "*.yml" "*.yaml" "*.json" "*.txt" "*.cfg" "*.ini" || true
            
            # Check if there are any staged changes
            if [ -n "$(git diff --cached --name-only)" ]; then
              # Final safety check - ensure no large files are being committed
              LARGE_FILES=$(find . -type f -size +10M | head -5)
              if [ -n "$LARGE_FILES" ]; then
                echo "âš ï¸ Warning: Large files detected:"
                echo "$LARGE_FILES"
                echo "Skipping commit to avoid large file issues."
                echo "branch=" >> "$GITHUB_OUTPUT"
                exit 0
              fi
              
              git commit -m "LLM: daily Spark optimizations"
              git push --set-upstream origin "$BR"
              echo "branch=$BR" >> "$GITHUB_OUTPUT"
            else
              echo "branch=" >> "$GITHUB_OUTPUT"
              echo "No meaningful changes to commit."
            fi
          else
            echo "branch=" >> "$GITHUB_OUTPUT"
            echo "No changes to commit."
          fi

      - name: Open PR (if branch exists)
        if: steps.commit.outputs.branch != ''
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          TITLE="LLM: Daily Spark optimizations ($(date +%Y-%m-%d))"
          BODY_FILE="llm_pr_body.md"
          if [ ! -f "$BODY_FILE" ]; then
            echo "Automated daily optimization suggestions." > "$BODY_FILE"
          fi
          gh pr create \
            --base main \
            --head "${{ steps.commit.outputs.branch }}" \
            --title "$TITLE" \
            --body-file "$BODY_FILE"
